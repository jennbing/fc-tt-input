function [net, info] = fcn_cifar10_autonn_full_input(varargin)
% FCN_CIFAR10_AUTONN_FULL_INPUT Train fully connected network on CIFAR10 
% data in full array.

% Fix the random seed.
rng(0);

opts.expDir = fullfile('data','cifar10-baseline') ;
[opts, varargin] = vl_argparse(opts, varargin) ;

opts.dataDir = fullfile('data','cifar10') ;
opts.imdbPath = fullfile(opts.expDir, 'imdb.mat');
opts.train.batchSize = 100;
opts.train.numEpochs = 100 ;
opts.train.continue = true ;
opts.train.gpus = [] ;
opts.train.learningRate = logspace(-2, -5, 45);
opts.train.expDir = opts.expDir ;
opts = vl_argparse(opts, varargin) ;

% --------------------------------------------------------------------
%                                                         Prepare data
% --------------------------------------------------------------------

if exist(opts.imdbPath, 'file')
  imdb = load(opts.imdbPath) ;
else
  imdb = getMnistImdb(opts) ;
  mkdir(opts.expDir) ;
  save(opts.imdbPath, '-struct', 'imdb') ;
end

  images = Input('gpu', true) ;
  labels = Input() ;

    inputModeSize = [4, 8, 3, 8, 4] ;
    secondModeSize = [7, 7, 7, 7, 7] ;
    ranks = [1, 2, 2, 2, 2, 1] ;
    
    W = tt_rand(secondModeSize.*inputModeSize, length(secondModeSize), ranks, []) ;
    W = tt_matrix(W, secondModeSize, inputModeSize) ;
    W.core = single(W.core) ;  
    
    x = vl_nntt_forward(images, 'W', W, ...
                           'weights', {W.core, zeros(1,1,prod(secondModeSize),'single')}, ...
                           'learningRate', [1, 2], ...
                           'weightDecay', [1, 0], ...
                           'outHeight', 1, ...
                           'outWidth', 1, ...
                           'outChannels', prod(secondModeSize)) ;
    x = vl_nnrelu(x) ;
    x = vl_nnconv(x, ... % 'weights', {{0.1*randn(1,1,prod(secondModeSize),10, 'single'), zeros(1, 10, 'single')}}, ...
                     'size', [1,1,prod(secondModeSize),10], ...
                     'learningRate', [1, 2], ...
                     'weightDecay', [1, 0], ...
                     'stride', 1, ...
                     'pad', 0) ;  
  
%   % diagnose outputs of conv layers
%   Layer.setDiagnostics(x.find(@vl_nnconv), true) ;

  % diagnose all Params associated with conv layers (1 depth up from them)
  convs = x.find(@vl_nnconv) ;
  convParams = cellfun(@(x) x.find('Param', 'depth', 1), convs, 'Uniform', false) ;
  Layer.setDiagnostics(convParams, true) ;
  
  objective = vl_nnloss(x, labels, 'loss', 'softmaxlog') ;
  error = vl_nnloss(x, labels, 'loss', 'classerror') ;

  Layer.workspaceNames() ;  % assign layer names based on workspace variables (e.g. 'images', 'objective')
  net = Net(objective, error) ;  % compile network
  
% --------------------------------------------------------------------
%                                                                Train
% --------------------------------------------------------------------

[net, info] = cnn_train_autonn(net, imdb, @getBatch, ...
  opts.train, 'val', find(imdb.images.set == 3)) ;

% --------------------------------------------------------------------
function [inputs] = getBatch(imdb, batch)
% --------------------------------------------------------------------

images = imdb.images.data(:,:,:,batch) ;
labels = imdb.images.labels(1,batch) ;
inputs = {'images', images, 'labels', labels} ;
%{
% Plot images before and after compression
images = imdb.images.data(:,batch,:) ;
labels = imdb.images.labels(1,batch) ;
images1 = tt_tensor(images, 0, [4 7 length(batch) 7 4]);
images1 = round(images1,1e-1,5);
images1 = reshape(full(images1),28,[],28);
figure();
for i=1:10
    subplot(2,10,i)
    for j=1:length(batch)
        if labels(j)==i
            imshow(reshape(images(:,j,:),28,28),[0 255]);
            subplot(2,10,i+10)
            imshow(reshape(images1(:,j,:),28,28),[0 255]);
            break;
        end
    end
end
saveas(gcf,'im_input.png');
%}

% --------------------------------------------------------------------
function imdb = getMnistImdb(opts)
% --------------------------------------------------------------------
% Prepare the imdb structure, returns image data with images of size 32 x 32
% and the mean image subtracted.
files = {'data_batch_1.mat', 'data_batch_2.mat', 'data_batch_3.mat', 'data_batch_4.mat', ...
         'data_batch_5.mat', 'test_batch.mat'};

if ~exist(opts.dataDir, 'dir')
  mkdir(opts.dataDir) ;
end

for i=1:length(files)
    if ~exist(fullfile(opts.dataDir, 'cifar-10-batches-mat', files{i}), 'file')
        url = 'https://www.cs.toronto.edu/~kriz/cifar-10-matlab.tar.gz';
        fprintf('downloading %s\n', url) ;
        untar(url, opts.dataDir);
    end
end

x1=[];
for i=1:5
    load(fullfile(opts.dataDir, 'cifar-10-batches-mat', sprintf('data_batch_%i.mat',i)));
    if i==1 
        x1=data;
        y1=labels;
    else
        x1=cat(1,x1,data);
        y1=cat(1,y1,labels);
    end
end

load(fullfile(opts.dataDir, 'cifar-10-batches-mat', 'test_batch.mat'));
x2=data; y2=labels;
y1=single(y1');y2=single(y2');

set = [ones(1,numel(y1)) 3*ones(1,numel(y2))] ;
dataSmall = single(reshape(cat(1, x1, x2),[],32,32,3)) ;
dataSmall = permute(dataSmall,[2,4,3,1]);

% Fill the image with zeros on the border to resize it to 32 x 32.
%data = zeros(32, size(dataSmall, 2), 32) ;
%data(3:30, :, 3:30) = dataSmall ;
dataMean = mean(dataSmall(:,:,:,set == 1), 4) ;
data = bsxfun(@minus, dataSmall, dataMean) ;
labels=cat(2,y1,y2);
% data = dataSmall;

imdb.images.data = data ;
imdb.images.data_mean = dataMean ;
% imdb.images.labels = cat(2, y1, y2) ;
imdb.images.labels = labels ; 
imdb.images.set = set ;
imdb.meta.sets = {'train', 'val', 'test'} ;
imdb.meta.classes = arrayfun(@(x)sprintf('%d',x),0:9,'uniformoutput',false) ;
